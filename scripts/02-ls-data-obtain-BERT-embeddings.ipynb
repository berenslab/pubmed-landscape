{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from random import randint\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import time\n",
    "\n",
    "import memory_profiler\n",
    "%load_ext memory_profiler\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import black\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load(line_length=79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pubmed_landscape_src.data import generate_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_path = Path(\"../results/variables\")\n",
    "figures_path = Path(\"../results/figures\")\n",
    "berenslab_data_path = Path(\"/gpfs01/berens/data/data/pubmed_processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 1073741824 bytes == 0x8c4e18000 @ \n",
      "tcmalloc: large alloc 1233903616 bytes == 0x96edbe000 @ \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 49300.59 MiB, increment: 48788.76 MiB\n",
      "CPU times: user 40.7 s, sys: 43.3 s, total: 1min 24s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# import clean_df_updated\n",
    "clean_df_updated_reparsed_filtered_with_authors_ISSN=pd.read_pickle(variables_path / \"clean_df_updated_reparsed_filtered_with_authors_ISSN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract abstract texts\n",
    "abstracts = clean_df_updated_reparsed_filtered_with_authors_ISSN['AbstractText'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMID</th>\n",
       "      <th>Title</th>\n",
       "      <th>AbstractText</th>\n",
       "      <th>Language</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Date</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1133452</td>\n",
       "      <td>Reflections on the special senses in relation ...</td>\n",
       "      <td>In congenital blindness, the absence of vision...</td>\n",
       "      <td>eng</td>\n",
       "      <td>Journal of the American Psychoanalytic Associa...</td>\n",
       "      <td>1975</td>\n",
       "      <td>pubmed21n0038.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1133453</td>\n",
       "      <td>Tracing a memory.</td>\n",
       "      <td>Comparisons of the nests of associations withi...</td>\n",
       "      <td>eng</td>\n",
       "      <td>Journal of the American Psychoanalytic Associa...</td>\n",
       "      <td>1975</td>\n",
       "      <td>pubmed21n0038.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1133454</td>\n",
       "      <td>Some pitfalls in the assessment of analyzabili...</td>\n",
       "      <td>We have described some of the problems encount...</td>\n",
       "      <td>eng</td>\n",
       "      <td>Journal of the American Psychoanalytic Associa...</td>\n",
       "      <td>1975</td>\n",
       "      <td>pubmed21n0038.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1133455</td>\n",
       "      <td>Variability of the iron, copper and mercury co...</td>\n",
       "      <td>The relative iron, copper and mercury contents...</td>\n",
       "      <td>eng</td>\n",
       "      <td>The journal of histochemistry and cytochemistr...</td>\n",
       "      <td>1975 May</td>\n",
       "      <td>pubmed21n0038.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1133526</td>\n",
       "      <td>Hypophysectomy of the tammar wallaby, Macropus...</td>\n",
       "      <td>A technique of hypophysectomy and regimes of p...</td>\n",
       "      <td>eng</td>\n",
       "      <td>The Journal of endocrinology</td>\n",
       "      <td>1975 Mar</td>\n",
       "      <td>pubmed21n0038.xml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      PMID                                              Title  \\\n",
       "0  1133452  Reflections on the special senses in relation ...   \n",
       "1  1133453                                  Tracing a memory.   \n",
       "2  1133454  Some pitfalls in the assessment of analyzabili...   \n",
       "3  1133455  Variability of the iron, copper and mercury co...   \n",
       "4  1133526  Hypophysectomy of the tammar wallaby, Macropus...   \n",
       "\n",
       "                                        AbstractText Language  \\\n",
       "0  In congenital blindness, the absence of vision...      eng   \n",
       "1  Comparisons of the nests of associations withi...      eng   \n",
       "2  We have described some of the problems encount...      eng   \n",
       "3  The relative iron, copper and mercury contents...      eng   \n",
       "4  A technique of hypophysectomy and regimes of p...      eng   \n",
       "\n",
       "                                             Journal      Date  \\\n",
       "0  Journal of the American Psychoanalytic Associa...      1975   \n",
       "1  Journal of the American Psychoanalytic Associa...      1975   \n",
       "2  Journal of the American Psychoanalytic Associa...      1975   \n",
       "3  The journal of histochemistry and cytochemistr...  1975 May   \n",
       "4                       The Journal of endocrinology  1975 Mar   \n",
       "\n",
       "            filename  \n",
       "0  pubmed21n0038.xml  \n",
       "1  pubmed21n0038.xml  \n",
       "2  pubmed21n0038.xml  \n",
       "3  pubmed21n0038.xml  \n",
       "4  pubmed21n0038.xml  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df_updated_reparsed_filtered_with_authors_ISSN.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining the PubMedBERT embeddings of the abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 49308.05 MiB, increment: 8.70 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "# specifying model \n",
    "checkpoint = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "#\"dmis-lab/biobert-v1.1\"\n",
    "\n",
    "#\"dmis-lab/biobert-v1.1\"\n",
    "#\"allenai/scibert_scivocab_uncased\"\n",
    "#\"bert-base-uncased\"\n",
    "#\"dmis-lab/biobert-v1.1\"\n",
    "#\"allenai/scibert_scivocab_uncased\"\n",
    "#\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "#\"dmis-lab/biobert-base-cased-v1.2\"\n",
    "#\"bert-base-uncased\"\n",
    "#\"allenai/scibert_scivocab_uncased\"\n",
    "#\"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "# specifying that calculations are performed on gpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "print(\"running on device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 1\n",
    "Batches of 5M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 30720000000 bytes == 0x7fabdcf20000 @ \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 78482.31 MiB, increment: 28828.03 MiB\n",
      "CPU times: user 15h 22min 27s, sys: 3h 47min 20s, total: 19h 9min 47s\n",
      "Wall time: 19h 22min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "mat = np.empty([5000000, 768])\n",
    "abstract_batch=abstracts[:5000000]\n",
    "for i, abst in enumerate(abstract_batch): \n",
    "    _, mat[i], _ = generate_embeddings(abst, tokenizer, model, device)\n",
    "    last_iter=np.array([i])\n",
    "    np.save('variables/last_iter_batch_1',last_iter)\n",
    "\n",
    "# save embedding\n",
    "np.save(berenslab_data_path / 'embeddings/embeddings_reparsed_batch_1', mat) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 30720000000 bytes == 0x7fa48af20000 @ \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 80344.27 MiB, increment: 30934.36 MiB\n",
      "CPU times: user 15h 10min 6s, sys: 4h 5min 1s, total: 19h 15min 7s\n",
      "Wall time: 19h 27min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "\n",
    "abstract_batch=abstracts[5000000:10000000]\n",
    "mat = np.empty([len(abstract_batch), 768])\n",
    "for i, abst in enumerate(abstract_batch): \n",
    "    _, mat[i], _ = generate_embeddings(abst, tokenizer, model, device)\n",
    "    last_iter=np.array([i+5000000])\n",
    "    np.save('variables/last_iter_batch_2',last_iter)\n",
    "    \n",
    "# save embedding\n",
    "np.save(berenslab_data_path / 'embeddings/embeddings_reparsed_batch_2', mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 30720000000 bytes == 0x7f8e17e3a000 @ \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "mat = np.empty([5000000, 768])\n",
    "abstract_batch=abstracts[10000000:15000000]\n",
    "for i, abst in enumerate(abstract_batch): \n",
    "    _, mat[i], _ = generate_embeddings(abst, tokenizer, model, device)\n",
    "    last_iter=np.array([i+10000000])\n",
    "    np.save('variables/last_iter_batch_3',last_iter)\n",
    "    \n",
    "# save embedding\n",
    "np.save(berenslab_data_path / 'embeddings/embeddings_reparsed_batch_3', mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "mat = np.empty([5000000, 768])\n",
    "abstract_batch=abstracts[15000000:20000000]\n",
    "for i, abst in enumerate(abstract_batch): \n",
    "    _, mat[i], _ = generate_embeddings(abst, tokenizer, model, device)\n",
    "    last_iter=np.array([i+15000000])\n",
    "    np.save('variables/last_iter_batch_4',last_iter)\n",
    "    \n",
    "# save embedding\n",
    "np.save(berenslab_data_path / 'embeddings/embeddings_reparsed_batch_4', mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "abstract_batch=abstracts[20000000:]\n",
    "mat = np.empty([len(abstract_batch), 768])\n",
    "for i, abst in enumerate(abstract_batch): \n",
    "    _, mat[i], _ = generate_embeddings(abst, tokenizer, model, device)\n",
    "    last_iter=np.array([i+20000000])\n",
    "    np.save('variables/last_iter_batch_5',last_iter)\n",
    "    \n",
    "# save embedding\n",
    "np.save(berenslab_data_path / 'embeddings/embeddings_reparsed_batch_5', mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Concatenate the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 30720000000 bytes == 0x7fa48af20000 @ \n"
     ]
    }
   ],
   "source": [
    "embeddings_reparsed_batch_1 = np.load(berenslab_data_path / 'embeddings/embeddings_reparsed_batch_1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 30720000000 bytes == 0x7f8e17e3a000 @ \n"
     ]
    }
   ],
   "source": [
    "embeddings_reparsed_batch_2 = np.load(berenslab_data_path / 'embeddings/embeddings_reparsed_batch_2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 30720000000 bytes == 0x7f953ef1a000 @ \n"
     ]
    }
   ],
   "source": [
    "embeddings_reparsed_batch_3 = np.load(berenslab_data_path / 'embeddings/embeddings_reparsed_batch_3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 30720000000 bytes == 0x7f9c65ffa000 @ \n"
     ]
    }
   ],
   "source": [
    "embeddings_reparsed_batch_4 = np.load(berenslab_data_path / 'embeddings/embeddings_reparsed_batch_4.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings_reparsed_batch_5 = np.load(berenslab_data_path / 'embeddings/embeddings_reparsed_batch_5.npy')\n",
    "embeddings_reparsed_batch_5 = mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 127101853696 bytes == 0x7f70781b4000 @ \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.5 s, sys: 30.5 s, total: 53 s\n",
      "Wall time: 53 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20687150, 768)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "embeddings_reparsed_all=np.vstack((embeddings_reparsed_batch_1, embeddings_reparsed_batch_2, embeddings_reparsed_batch_3, embeddings_reparsed_batch_4, embeddings_reparsed_batch_5))\n",
    "embeddings_reparsed_all.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.183px",
    "left": "910px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
